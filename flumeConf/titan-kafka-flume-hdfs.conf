a3.sources = r3
a3.channels = c3
a3.sinks = k3

# source
a3.sources.r3.type = org.apache.flume.source.kafka.KafkaSource
a3.sources.r3.batchSize = 5000
a3.sources.r3.batchDurationMillis = 2000
a3.sources.r3.kafka.bootstrap.servers = master:9092,slave1:9092,slave2:9092
a3.sources.r3.kafka.topics = titan-log-topic
a3.sources.r3.kafka.consumer.group.id = titan-flume-consumer

#channel
#a3.channels.c3.type = memory
#a3.channels.c3.capacity = 10000
#a3.channels.c3.transactionCapacity = 10000
#a3.channels.c3.byteCapacityBufferPercentage = 20
#a3.channels.c3.byteCapacity = 6912212
a3.channels.c3.type = file
a3.channels.c3.checkpointDir = /BigData/flume-1.9.0/data/checkpoint
a3.channels.c3.dataDirs = /BigData/flume-1.9.0/data/realData
a3.channels.c3.maxFileSize = 2146435071
a3.channels.c3.capacity = 1000000
a3.channels.c3.keep-alive = 6

#sink
a3.sinks.k3.type = hdfs
a3.sinks.k3.hdfs.path = /data/titan/events_logs/%Y-%m-%d/
#上传文件前缀
a3.sinks.k3.hdfs.filePrefix = events-
# 设置压缩格式
a3.sinks.k3.hdfs.fileType = CompressedStream
a3.sinks.k3.hdfs.codeC = lzop
#不要产生大量小文件
a3.sinks.k3.hdfs.rollInterval = 3600
#刚好是128M 
a3.sinks.k3.hdfs.rollSize = 134217728
a3.sinks.k3.hdfs.rollCount = 0

#拼装
a3.sources.r3.channels = c3
a3.sinks.k3.channel = c3
